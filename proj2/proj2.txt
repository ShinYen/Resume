1. 
6 Clusters: 44 minutes, 22 seconds. 3844 Mappers, 1588 Reducers
9 Clusters: 58 minutes 53 seconds. 4322 Mappers, 1802 Reducers
12 Clusters: 42 minutes 24 seconds. 5762 Mappers, 2354 Reducers

2.
 6 Clusters: 6122.99122 MB of output size, 2902 seconds so 6122.99122/2902 = 2.11 MB/s
9 Clusters: 6123.23054 MB output size, 3233 seconds. so 6123.23054/3233 = .1.89 MB/s
12 Clusters: 6123.4714 MB output size, 2544 seconds, so 6123.4714/2544 = 2.41 MB/s

3. There is no consistent speedup between 9 and 12 versus 6. Hadoop didn’t parallelize our code well because even though we are adding more workers, we aren’t seeing any speed increases. We have strong scaling because we have a set amount of data size and varying workers

4. 
InitFirst: There would be no sense in making a combiner here because there is only one operation to complete which is to make a blank board.

Possible Moves: It would be possible to combine all the boards and add up their parent moves. The combiner would only be helpful if in that layer, theres are multiple identical boards. This does not happen that often and therefore woulld not offer that much speed up.

Solve Moves: Including a combiner here would increase the speed because values from possibleMoves and the solvedmoves mappers are passed into the reducer. The combiner would be useful because it could potentially cut the ammount of time reducer is called by half. 

Final Moves: No combiner is necessary because there is only one val for each board state. So a combiner is unnecesary

5.
6 Clusters: 6 *  5.97948/.68 = .68 dollar/GB
9 Clusters:  9 * 5.9797/.68 = 1.02 Dollar/GB
12 Clusters: 12 * 5.980/.68 = 1.36 Dollar/GB

6. 
6 Clusters: 6 * .68 * 4 hours = 16.32
9 Clusters: 9 * .68 *  3 hours = 18.36
12 Clusters: 12 * .68 * 2 hours = 16.32
Total = $51